{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b38ae9a-9243-4cb1-8972-8e4f0042ef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "Training Example 1: u = (1/(2π²)) sin(πx) sin(πy) with P4 BCs\n",
      "================================================================================\n",
      "Epoch     0 | Total: 9.450e+01 | Int: 8.995e+01 | BC: 4.551e+00 | L2: 4.322e-01 | Energy: 9.489e-01 | LR: 1.00e-03\n",
      "Epoch   100 | Total: 1.808e+00 | Int: 1.213e+00 | BC: 5.951e-01 | L2: 2.948e-01 | Energy: 9.516e-01 | LR: 1.00e-03\n",
      "Epoch   200 | Total: 5.658e-01 | Int: 3.186e-01 | BC: 2.472e-01 | L2: 5.184e-01 | Energy: 6.908e-01 | LR: 1.00e-03\n",
      "Epoch   300 | Total: 1.233e-01 | Int: 8.710e-02 | BC: 3.620e-02 | L2: 1.549e-01 | Energy: 2.211e-01 | LR: 1.00e-03\n",
      "Epoch   400 | Total: 8.053e-02 | Int: 6.377e-02 | BC: 1.676e-02 | L2: 4.491e-01 | Energy: 4.959e-01 | LR: 1.00e-03\n",
      "Epoch   500 | Total: 5.749e-02 | Int: 4.644e-02 | BC: 1.104e-02 | L2: 6.275e-01 | Energy: 6.584e-01 | LR: 1.00e-03\n",
      "Epoch   600 | Total: 3.724e-02 | Int: 3.010e-02 | BC: 7.138e-03 | L2: 7.535e-01 | Energy: 7.820e-01 | LR: 1.00e-03\n",
      "Epoch   700 | Total: 2.396e-02 | Int: 1.734e-02 | BC: 6.616e-03 | L2: 8.283e-01 | Energy: 8.619e-01 | LR: 1.00e-03\n",
      "Epoch   800 | Total: 1.955e-02 | Int: 1.343e-02 | BC: 6.116e-03 | L2: 8.706e-01 | Energy: 9.079e-01 | LR: 1.00e-03\n",
      "Epoch   900 | Total: 1.670e-02 | Int: 1.089e-02 | BC: 5.815e-03 | L2: 8.988e-01 | Energy: 9.394e-01 | LR: 1.00e-03\n",
      "Epoch  1000 | Total: 3.195e-02 | Int: 2.489e-02 | BC: 7.054e-03 | L2: 9.097e-01 | Energy: 9.504e-01 | LR: 1.00e-03\n",
      "Epoch  1100 | Total: 2.292e-02 | Int: 1.526e-02 | BC: 7.660e-03 | L2: 9.192e-01 | Energy: 9.594e-01 | LR: 1.00e-03\n",
      "Epoch  1200 | Total: 1.249e-02 | Int: 7.156e-03 | BC: 5.334e-03 | L2: 9.281e-01 | Energy: 9.744e-01 | LR: 1.00e-03\n",
      "Epoch  1300 | Total: 1.071e-02 | Int: 5.711e-03 | BC: 5.004e-03 | L2: 9.289e-01 | Energy: 9.749e-01 | LR: 1.00e-03\n",
      "Epoch  1400 | Total: 8.066e-02 | Int: 6.853e-02 | BC: 1.213e-02 | L2: 9.017e-01 | Energy: 9.374e-01 | LR: 1.00e-03\n",
      "Epoch  1500 | Total: 7.025e-02 | Int: 6.342e-02 | BC: 6.825e-03 | L2: 9.161e-01 | Energy: 9.629e-01 | LR: 1.00e-03\n",
      "Epoch  1600 | Total: 2.834e-01 | Int: 2.679e-01 | BC: 1.547e-02 | L2: 8.814e-01 | Energy: 9.210e-01 | LR: 1.00e-03\n",
      "Epoch  1700 | Total: 1.463e-02 | Int: 1.012e-02 | BC: 4.509e-03 | L2: 8.891e-01 | Energy: 9.363e-01 | LR: 1.00e-03\n",
      "Epoch  1800 | Total: 1.188e-02 | Int: 8.141e-03 | BC: 3.742e-03 | L2: 8.775e-01 | Energy: 9.239e-01 | LR: 1.00e-03\n",
      "Epoch  1900 | Total: 1.042e-02 | Int: 6.918e-03 | BC: 3.499e-03 | L2: 8.573e-01 | Energy: 9.031e-01 | LR: 1.00e-03\n",
      "Epoch  2000 | Total: 8.179e-03 | Int: 4.578e-03 | BC: 3.600e-03 | L2: 8.476e-01 | Energy: 8.928e-01 | LR: 1.00e-03\n",
      "Epoch  2100 | Total: 6.923e-03 | Int: 3.665e-03 | BC: 3.259e-03 | L2: 8.218e-01 | Energy: 8.662e-01 | LR: 1.00e-03\n",
      "Epoch  2200 | Total: 1.229e-01 | Int: 1.190e-01 | BC: 3.937e-03 | L2: 8.139e-01 | Energy: 8.571e-01 | LR: 1.00e-03\n",
      "Epoch  2300 | Total: 1.326e-02 | Int: 9.520e-03 | BC: 3.744e-03 | L2: 7.857e-01 | Energy: 8.255e-01 | LR: 1.00e-03\n",
      "Epoch  2400 | Total: 2.427e-02 | Int: 2.100e-02 | BC: 3.271e-03 | L2: 7.734e-01 | Energy: 8.144e-01 | LR: 1.00e-03\n",
      "Epoch  2500 | Total: 3.638e-02 | Int: 3.325e-02 | BC: 3.128e-03 | L2: 7.490e-01 | Energy: 7.878e-01 | LR: 1.00e-03\n",
      "Epoch  2600 | Total: 1.561e-02 | Int: 1.299e-02 | BC: 2.617e-03 | L2: 7.407e-01 | Energy: 7.803e-01 | LR: 1.00e-03\n",
      "Epoch  2700 | Total: 5.034e-03 | Int: 2.582e-03 | BC: 2.452e-03 | L2: 7.229e-01 | Energy: 7.621e-01 | LR: 1.00e-03\n",
      "Epoch  2800 | Total: 4.475e-03 | Int: 2.147e-03 | BC: 2.327e-03 | L2: 7.138e-01 | Energy: 7.528e-01 | LR: 1.00e-03\n",
      "Epoch  2900 | Total: 7.444e-02 | Int: 6.860e-02 | BC: 5.833e-03 | L2: 6.861e-01 | Energy: 7.191e-01 | LR: 1.00e-03\n",
      "Epoch  3000 | Total: 1.001e-02 | Int: 7.906e-03 | BC: 2.104e-03 | L2: 6.823e-01 | Energy: 7.206e-01 | LR: 1.00e-03\n",
      "Epoch  3100 | Total: 2.329e-02 | Int: 2.129e-02 | BC: 1.996e-03 | L2: 6.690e-01 | Energy: 7.063e-01 | LR: 1.00e-03\n",
      "Epoch  3200 | Total: 7.947e-03 | Int: 6.107e-03 | BC: 1.840e-03 | L2: 6.515e-01 | Energy: 6.881e-01 | LR: 1.00e-03\n",
      "Epoch  3300 | Total: 6.249e-03 | Int: 4.146e-03 | BC: 2.104e-03 | L2: 6.465e-01 | Energy: 6.820e-01 | LR: 1.00e-03\n",
      "Epoch  3400 | Total: 5.007e-03 | Int: 3.257e-03 | BC: 1.750e-03 | L2: 6.254e-01 | Energy: 6.606e-01 | LR: 1.00e-03\n",
      "Epoch  3500 | Total: 1.256e-02 | Int: 1.043e-02 | BC: 2.130e-03 | L2: 6.158e-01 | Energy: 6.502e-01 | LR: 1.00e-03\n",
      "Epoch  3600 | Total: 9.070e-03 | Int: 7.083e-03 | BC: 1.986e-03 | L2: 5.923e-01 | Energy: 6.245e-01 | LR: 1.00e-03\n",
      "Epoch  3700 | Total: 1.458e-01 | Int: 1.409e-01 | BC: 4.949e-03 | L2: 5.772e-01 | Energy: 6.086e-01 | LR: 1.00e-03\n",
      "Epoch  3800 | Total: 3.353e-03 | Int: 1.856e-03 | BC: 1.497e-03 | L2: 5.775e-01 | Energy: 6.106e-01 | LR: 1.00e-03\n",
      "Epoch  3900 | Total: 1.393e-02 | Int: 1.248e-02 | BC: 1.449e-03 | L2: 5.590e-01 | Energy: 5.926e-01 | LR: 1.00e-03\n",
      "Epoch  4000 | Total: 8.168e-03 | Int: 6.581e-03 | BC: 1.587e-03 | L2: 5.561e-01 | Energy: 5.896e-01 | LR: 1.00e-03\n",
      "Epoch  4100 | Total: 7.381e-03 | Int: 5.419e-03 | BC: 1.962e-03 | L2: 5.357e-01 | Energy: 5.657e-01 | LR: 1.00e-03\n",
      "Epoch  4200 | Total: 1.664e-02 | Int: 1.527e-02 | BC: 1.377e-03 | L2: 5.286e-01 | Energy: 5.594e-01 | LR: 1.00e-03\n",
      "Epoch  4300 | Total: 7.612e-02 | Int: 7.434e-02 | BC: 1.785e-03 | L2: 5.209e-01 | Energy: 5.512e-01 | LR: 1.00e-03\n",
      "Epoch  4400 | Total: 2.118e-02 | Int: 1.968e-02 | BC: 1.505e-03 | L2: 5.079e-01 | Energy: 5.370e-01 | LR: 1.00e-03\n",
      "Epoch  4500 | Total: 5.050e-03 | Int: 3.563e-03 | BC: 1.487e-03 | L2: 4.965e-01 | Energy: 5.258e-01 | LR: 1.00e-03\n",
      "Epoch  4600 | Total: 1.238e-02 | Int: 1.124e-02 | BC: 1.143e-03 | L2: 4.837e-01 | Energy: 5.128e-01 | LR: 1.00e-03\n",
      "Epoch  4700 | Total: 5.703e-03 | Int: 4.592e-03 | BC: 1.111e-03 | L2: 4.798e-01 | Energy: 5.095e-01 | LR: 1.00e-03\n",
      "Epoch  4800 | Total: 1.659e-02 | Int: 1.514e-02 | BC: 1.449e-03 | L2: 4.706e-01 | Energy: 5.001e-01 | LR: 1.00e-03\n",
      "Epoch  4900 | Total: 4.948e-03 | Int: 3.895e-03 | BC: 1.053e-03 | L2: 4.535e-01 | Energy: 4.817e-01 | LR: 1.00e-03\n",
      "Epoch  5000 | Total: 8.357e-03 | Int: 7.223e-03 | BC: 1.134e-03 | L2: 4.534e-01 | Energy: 4.825e-01 | LR: 1.00e-03\n",
      "Epoch  5100 | Total: 8.392e-03 | Int: 7.202e-03 | BC: 1.191e-03 | L2: 4.392e-01 | Energy: 4.670e-01 | LR: 1.00e-03\n",
      "Epoch  5200 | Total: 3.147e-02 | Int: 3.027e-02 | BC: 1.197e-03 | L2: 4.414e-01 | Energy: 4.695e-01 | LR: 1.00e-03\n",
      "Epoch  5300 | Total: 4.686e-03 | Int: 3.766e-03 | BC: 9.203e-04 | L2: 4.211e-01 | Energy: 4.496e-01 | LR: 1.00e-03\n",
      "Epoch  5400 | Total: 2.366e-02 | Int: 2.173e-02 | BC: 1.934e-03 | L2: 4.249e-01 | Energy: 4.524e-01 | LR: 1.00e-03\n",
      "Epoch  5500 | Total: 7.821e-03 | Int: 6.909e-03 | BC: 9.129e-04 | L2: 4.088e-01 | Energy: 4.366e-01 | LR: 1.00e-03\n",
      "Epoch  5600 | Total: 4.792e-03 | Int: 3.482e-03 | BC: 1.310e-03 | L2: 4.005e-01 | Energy: 4.281e-01 | LR: 1.00e-03\n",
      "Epoch  5700 | Total: 7.923e-03 | Int: 7.024e-03 | BC: 8.992e-04 | L2: 3.933e-01 | Energy: 4.211e-01 | LR: 1.00e-03\n",
      "Epoch  5800 | Total: 6.351e-02 | Int: 5.992e-02 | BC: 3.588e-03 | L2: 3.704e-01 | Energy: 3.934e-01 | LR: 1.00e-03\n",
      "Epoch  5900 | Total: 3.142e-03 | Int: 2.229e-03 | BC: 9.127e-04 | L2: 3.852e-01 | Energy: 4.127e-01 | LR: 1.00e-03\n",
      "Epoch  6000 | Total: 1.058e-02 | Int: 9.618e-03 | BC: 9.651e-04 | L2: 3.641e-01 | Energy: 3.896e-01 | LR: 1.00e-03\n",
      "Epoch  6100 | Total: 6.771e-03 | Int: 5.278e-03 | BC: 1.494e-03 | L2: 3.617e-01 | Energy: 3.851e-01 | LR: 1.00e-03\n",
      "Epoch  6200 | Total: 2.257e-03 | Int: 1.401e-03 | BC: 8.551e-04 | L2: 3.590e-01 | Energy: 3.851e-01 | LR: 1.00e-03\n",
      "Epoch  6300 | Total: 1.032e-02 | Int: 9.254e-03 | BC: 1.067e-03 | L2: 3.444e-01 | Energy: 3.693e-01 | LR: 1.00e-03\n",
      "Epoch  6400 | Total: 4.324e-03 | Int: 3.507e-03 | BC: 8.175e-04 | L2: 3.454e-01 | Energy: 3.719e-01 | LR: 1.00e-03\n",
      "Epoch  6500 | Total: 3.205e-03 | Int: 2.264e-03 | BC: 9.410e-04 | L2: 3.380e-01 | Energy: 3.634e-01 | LR: 1.00e-03\n",
      "Epoch  6600 | Total: 9.324e-03 | Int: 7.610e-03 | BC: 1.714e-03 | L2: 3.273e-01 | Energy: 3.509e-01 | LR: 1.00e-03\n",
      "Epoch  6700 | Total: 1.198e-02 | Int: 1.061e-02 | BC: 1.375e-03 | L2: 3.167e-01 | Energy: 3.406e-01 | LR: 1.00e-03\n",
      "Epoch  6800 | Total: 1.528e-03 | Int: 8.611e-04 | BC: 6.671e-04 | L2: 3.119e-01 | Energy: 3.369e-01 | LR: 5.00e-04\n",
      "Epoch  6900 | Total: 1.432e-03 | Int: 8.408e-04 | BC: 5.917e-04 | L2: 2.986e-01 | Energy: 3.233e-01 | LR: 5.00e-04\n",
      "Epoch  7000 | Total: 1.379e-03 | Int: 8.313e-04 | BC: 5.477e-04 | L2: 2.883e-01 | Energy: 3.126e-01 | LR: 5.00e-04\n",
      "Epoch  7100 | Total: 1.340e-03 | Int: 8.205e-04 | BC: 5.196e-04 | L2: 2.800e-01 | Energy: 3.039e-01 | LR: 5.00e-04\n",
      "Epoch  7200 | Total: 1.307e-03 | Int: 8.073e-04 | BC: 4.993e-04 | L2: 2.729e-01 | Energy: 2.964e-01 | LR: 5.00e-04\n",
      "Epoch  7300 | Total: 1.276e-03 | Int: 7.921e-04 | BC: 4.834e-04 | L2: 2.664e-01 | Energy: 2.896e-01 | LR: 5.00e-04\n",
      "Epoch  7400 | Total: 1.246e-03 | Int: 7.760e-04 | BC: 4.698e-04 | L2: 2.603e-01 | Energy: 2.833e-01 | LR: 5.00e-04\n",
      "Epoch  7500 | Total: 1.217e-03 | Int: 7.593e-04 | BC: 4.577e-04 | L2: 2.545e-01 | Energy: 2.771e-01 | LR: 5.00e-04\n",
      "Epoch  7600 | Total: 1.189e-03 | Int: 7.424e-04 | BC: 4.466e-04 | L2: 2.487e-01 | Energy: 2.711e-01 | LR: 5.00e-04\n",
      "Epoch  7700 | Total: 1.162e-03 | Int: 7.256e-04 | BC: 4.362e-04 | L2: 2.430e-01 | Energy: 2.650e-01 | LR: 5.00e-04\n",
      "Epoch  7800 | Total: 1.135e-03 | Int: 7.088e-04 | BC: 4.263e-04 | L2: 2.372e-01 | Energy: 2.590e-01 | LR: 5.00e-04\n",
      "Epoch  7900 | Total: 1.109e-03 | Int: 6.921e-04 | BC: 4.168e-04 | L2: 2.314e-01 | Energy: 2.529e-01 | LR: 5.00e-04\n",
      "Epoch  8000 | Total: 1.083e-03 | Int: 6.756e-04 | BC: 4.076e-04 | L2: 2.254e-01 | Energy: 2.467e-01 | LR: 5.00e-04\n",
      "Epoch  8100 | Total: 1.172e-03 | Int: 7.268e-04 | BC: 4.448e-04 | L2: 2.294e-01 | Energy: 2.506e-01 | LR: 5.00e-04\n",
      "Epoch  8200 | Total: 2.864e-03 | Int: 2.401e-03 | BC: 4.631e-04 | L2: 2.246e-01 | Energy: 2.466e-01 | LR: 5.00e-04\n",
      "Epoch  8300 | Total: 2.929e-03 | Int: 2.495e-03 | BC: 4.342e-04 | L2: 2.187e-01 | Energy: 2.400e-01 | LR: 5.00e-04\n",
      "Epoch  8400 | Total: 1.050e-03 | Int: 6.373e-04 | BC: 4.127e-04 | L2: 2.141e-01 | Energy: 2.352e-01 | LR: 5.00e-04\n",
      "Epoch  8500 | Total: 1.045e-03 | Int: 6.093e-04 | BC: 4.359e-04 | L2: 2.151e-01 | Energy: 2.362e-01 | LR: 5.00e-04\n",
      "Epoch  8600 | Total: 1.148e-03 | Int: 7.287e-04 | BC: 4.197e-04 | L2: 2.090e-01 | Energy: 2.300e-01 | LR: 5.00e-04\n",
      "Epoch  8700 | Total: 1.676e-03 | Int: 1.217e-03 | BC: 4.592e-04 | L2: 2.005e-01 | Energy: 2.208e-01 | LR: 5.00e-04\n",
      "Epoch  8800 | Total: 1.239e-03 | Int: 8.330e-04 | BC: 4.056e-04 | L2: 1.994e-01 | Energy: 2.204e-01 | LR: 5.00e-04\n",
      "Epoch  8900 | Total: 1.481e-03 | Int: 1.059e-03 | BC: 4.217e-04 | L2: 1.959e-01 | Energy: 2.166e-01 | LR: 5.00e-04\n",
      "Epoch  9000 | Total: 9.728e-04 | Int: 5.800e-04 | BC: 3.928e-04 | L2: 1.883e-01 | Energy: 2.087e-01 | LR: 5.00e-04\n",
      "Epoch  9100 | Total: 1.046e-03 | Int: 6.480e-04 | BC: 3.975e-04 | L2: 1.879e-01 | Energy: 2.084e-01 | LR: 5.00e-04\n",
      "Epoch  9200 | Total: 2.860e-03 | Int: 2.447e-03 | BC: 4.128e-04 | L2: 1.804e-01 | Energy: 2.009e-01 | LR: 5.00e-04\n",
      "Epoch  9300 | Total: 1.055e-03 | Int: 6.503e-04 | BC: 4.051e-04 | L2: 1.775e-01 | Energy: 1.975e-01 | LR: 5.00e-04\n",
      "Epoch  9400 | Total: 6.964e-02 | Int: 6.845e-02 | BC: 1.193e-03 | L2: 1.739e-01 | Energy: 1.956e-01 | LR: 5.00e-04\n",
      "Epoch  9500 | Total: 9.628e-04 | Int: 5.767e-04 | BC: 3.861e-04 | L2: 1.698e-01 | Energy: 1.898e-01 | LR: 5.00e-04\n",
      "Epoch  9600 | Total: 2.063e-02 | Int: 2.008e-02 | BC: 5.447e-04 | L2: 1.668e-01 | Energy: 1.868e-01 | LR: 5.00e-04\n",
      "Epoch  9700 | Total: 2.863e-03 | Int: 2.315e-03 | BC: 5.479e-04 | L2: 1.622e-01 | Energy: 1.809e-01 | LR: 5.00e-04\n",
      "Epoch  9800 | Total: 2.267e-03 | Int: 1.807e-03 | BC: 4.597e-04 | L2: 1.580e-01 | Energy: 1.777e-01 | LR: 5.00e-04\n",
      "Epoch  9900 | Total: 2.581e-03 | Int: 2.161e-03 | BC: 4.207e-04 | L2: 1.527e-01 | Energy: 1.720e-01 | LR: 5.00e-04\n",
      "Epoch 10000 | Total: 1.246e-03 | Int: 8.710e-04 | BC: 3.746e-04 | L2: 1.501e-01 | Energy: 1.696e-01 | LR: 5.00e-04\n",
      "Epoch 10100 | Total: 2.352e-02 | Int: 2.240e-02 | BC: 1.119e-03 | L2: 1.476e-01 | Energy: 1.684e-01 | LR: 5.00e-04\n",
      "Epoch 10200 | Total: 8.939e-04 | Int: 5.302e-04 | BC: 3.637e-04 | L2: 1.435e-01 | Energy: 1.630e-01 | LR: 5.00e-04\n",
      "Epoch 10300 | Total: 1.289e-02 | Int: 1.244e-02 | BC: 4.493e-04 | L2: 1.379e-01 | Energy: 1.568e-01 | LR: 5.00e-04\n",
      "Epoch 10400 | Total: 1.088e-03 | Int: 7.394e-04 | BC: 3.489e-04 | L2: 1.344e-01 | Energy: 1.538e-01 | LR: 5.00e-04\n",
      "Epoch 10500 | Total: 9.365e-04 | Int: 5.871e-04 | BC: 3.494e-04 | L2: 1.320e-01 | Energy: 1.511e-01 | LR: 5.00e-04\n",
      "Epoch 10600 | Total: 8.251e-04 | Int: 4.814e-04 | BC: 3.437e-04 | L2: 1.275e-01 | Energy: 1.464e-01 | LR: 5.00e-04\n",
      "Epoch 10700 | Total: 9.152e-04 | Int: 5.673e-04 | BC: 3.479e-04 | L2: 1.228e-01 | Energy: 1.415e-01 | LR: 5.00e-04\n",
      "Epoch 10800 | Total: 8.745e-04 | Int: 4.894e-04 | BC: 3.851e-04 | L2: 1.234e-01 | Energy: 1.422e-01 | LR: 5.00e-04\n",
      "Epoch 10900 | Total: 2.807e-03 | Int: 2.291e-03 | BC: 5.158e-04 | L2: 1.192e-01 | Energy: 1.372e-01 | LR: 5.00e-04\n",
      "Epoch 11000 | Total: 3.068e-03 | Int: 2.640e-03 | BC: 4.277e-04 | L2: 1.131e-01 | Energy: 1.313e-01 | LR: 5.00e-04\n",
      "Epoch 11100 | Total: 9.003e-04 | Int: 5.638e-04 | BC: 3.364e-04 | L2: 1.126e-01 | Energy: 1.316e-01 | LR: 5.00e-04\n",
      "Epoch 11200 | Total: 5.121e-03 | Int: 4.750e-03 | BC: 3.712e-04 | L2: 1.057e-01 | Energy: 1.239e-01 | LR: 5.00e-04\n",
      "Epoch 11300 | Total: 7.415e-03 | Int: 6.835e-03 | BC: 5.801e-04 | L2: 1.044e-01 | Energy: 1.239e-01 | LR: 5.00e-04\n",
      "Epoch 11400 | Total: 5.323e-03 | Int: 4.973e-03 | BC: 3.503e-04 | L2: 1.002e-01 | Energy: 1.188e-01 | LR: 5.00e-04\n",
      "Epoch 11500 | Total: 1.787e-03 | Int: 1.445e-03 | BC: 3.416e-04 | L2: 9.903e-02 | Energy: 1.173e-01 | LR: 5.00e-04\n",
      "Epoch 11600 | Total: 9.441e-04 | Int: 6.143e-04 | BC: 3.298e-04 | L2: 9.489e-02 | Energy: 1.131e-01 | LR: 5.00e-04\n",
      "Epoch 11700 | Total: 2.061e-03 | Int: 1.701e-03 | BC: 3.598e-04 | L2: 8.971e-02 | Energy: 1.077e-01 | LR: 5.00e-04\n",
      "Epoch 11800 | Total: 1.403e-03 | Int: 1.053e-03 | BC: 3.501e-04 | L2: 8.881e-02 | Energy: 1.065e-01 | LR: 5.00e-04\n",
      "Epoch 11900 | Total: 4.994e-03 | Int: 4.563e-03 | BC: 4.304e-04 | L2: 8.550e-02 | Energy: 1.043e-01 | LR: 5.00e-04\n",
      "Epoch 12000 | Total: 2.433e-03 | Int: 1.995e-03 | BC: 4.379e-04 | L2: 8.461e-02 | Energy: 1.021e-01 | LR: 5.00e-04\n",
      "Epoch 12100 | Total: 1.041e-03 | Int: 7.142e-04 | BC: 3.266e-04 | L2: 7.823e-02 | Energy: 9.573e-02 | LR: 5.00e-04\n",
      "Epoch 12200 | Total: 7.404e-04 | Int: 4.315e-04 | BC: 3.088e-04 | L2: 7.476e-02 | Energy: 9.239e-02 | LR: 5.00e-04\n",
      "Epoch 12300 | Total: 7.178e-04 | Int: 4.042e-04 | BC: 3.136e-04 | L2: 7.522e-02 | Energy: 9.308e-02 | LR: 5.00e-04\n",
      "Epoch 12400 | Total: 7.127e-04 | Int: 4.109e-04 | BC: 3.018e-04 | L2: 6.861e-02 | Energy: 8.618e-02 | LR: 5.00e-04\n",
      "Epoch 12500 | Total: 7.487e-04 | Int: 4.399e-04 | BC: 3.088e-04 | L2: 6.800e-02 | Energy: 8.572e-02 | LR: 5.00e-04\n",
      "Epoch 12600 | Total: 8.767e-03 | Int: 8.332e-03 | BC: 4.354e-04 | L2: 6.634e-02 | Energy: 8.316e-02 | LR: 5.00e-04\n",
      "Epoch 12700 | Total: 2.047e-03 | Int: 1.692e-03 | BC: 3.544e-04 | L2: 6.116e-02 | Energy: 7.820e-02 | LR: 5.00e-04\n",
      "Epoch 12800 | Total: 1.798e-03 | Int: 1.482e-03 | BC: 3.153e-04 | L2: 5.794e-02 | Energy: 7.511e-02 | LR: 5.00e-04\n",
      "Epoch 12900 | Total: 9.054e-03 | Int: 8.549e-03 | BC: 5.059e-04 | L2: 5.879e-02 | Energy: 7.604e-02 | LR: 5.00e-04\n",
      "Epoch 13000 | Total: 3.850e-02 | Int: 3.663e-02 | BC: 1.862e-03 | L2: 5.509e-02 | Energy: 7.599e-02 | LR: 5.00e-04\n",
      "Epoch 13100 | Total: 2.892e-03 | Int: 2.512e-03 | BC: 3.795e-04 | L2: 5.318e-02 | Energy: 7.040e-02 | LR: 5.00e-04\n",
      "Epoch 13200 | Total: 1.145e-03 | Int: 8.109e-04 | BC: 3.340e-04 | L2: 4.502e-02 | Energy: 6.150e-02 | LR: 5.00e-04\n",
      "Epoch 13300 | Total: 6.721e-04 | Int: 3.730e-04 | BC: 2.991e-04 | L2: 4.715e-02 | Energy: 6.433e-02 | LR: 5.00e-04\n",
      "Epoch 13400 | Total: 1.687e-02 | Int: 1.587e-02 | BC: 9.998e-04 | L2: 3.476e-02 | Energy: 5.033e-02 | LR: 5.00e-04\n",
      "Epoch 13500 | Total: 2.430e-02 | Int: 2.375e-02 | BC: 5.555e-04 | L2: 4.121e-02 | Energy: 5.860e-02 | LR: 5.00e-04\n",
      "Epoch 13600 | Total: 6.269e-04 | Int: 3.440e-04 | BC: 2.828e-04 | L2: 3.535e-02 | Energy: 5.231e-02 | LR: 5.00e-04\n",
      "Epoch 13700 | Total: 1.430e-02 | Int: 1.393e-02 | BC: 3.756e-04 | L2: 3.227e-02 | Energy: 4.913e-02 | LR: 5.00e-04\n",
      "Epoch 13800 | Total: 6.415e-04 | Int: 3.666e-04 | BC: 2.750e-04 | L2: 2.840e-02 | Energy: 4.518e-02 | LR: 5.00e-04\n",
      "Epoch 13900 | Total: 1.699e-03 | Int: 1.386e-03 | BC: 3.135e-04 | L2: 2.672e-02 | Energy: 4.317e-02 | LR: 5.00e-04\n",
      "Epoch 14000 | Total: 9.073e-04 | Int: 6.352e-04 | BC: 2.721e-04 | L2: 2.324e-02 | Energy: 3.986e-02 | LR: 5.00e-04\n",
      "Epoch 14100 | Total: 1.729e-02 | Int: 1.675e-02 | BC: 5.482e-04 | L2: 2.203e-02 | Energy: 3.947e-02 | LR: 5.00e-04\n",
      "Epoch 14200 | Total: 7.281e-04 | Int: 4.546e-04 | BC: 2.735e-04 | L2: 1.998e-02 | Energy: 3.669e-02 | LR: 5.00e-04\n",
      "Epoch 14300 | Total: 1.370e-03 | Int: 1.047e-03 | BC: 3.225e-04 | L2: 1.905e-02 | Energy: 3.537e-02 | LR: 2.50e-04\n",
      "Epoch 14400 | Total: 5.825e-04 | Int: 3.151e-04 | BC: 2.674e-04 | L2: 1.399e-02 | Energy: 3.037e-02 | LR: 2.50e-04\n",
      "Epoch 14500 | Total: 5.733e-04 | Int: 3.127e-04 | BC: 2.606e-04 | L2: 1.010e-02 | Energy: 2.634e-02 | LR: 2.50e-04\n",
      "Epoch 14600 | Total: 5.666e-04 | Int: 3.104e-04 | BC: 2.562e-04 | L2: 7.002e-03 | Energy: 2.311e-02 | LR: 2.50e-04\n",
      "Epoch 14700 | Total: 5.607e-04 | Int: 3.078e-04 | BC: 2.529e-04 | L2: 4.454e-03 | Energy: 2.044e-02 | LR: 2.50e-04\n",
      "Epoch 14800 | Total: 5.550e-04 | Int: 3.049e-04 | BC: 2.502e-04 | L2: 2.744e-03 | Energy: 1.861e-02 | LR: 2.50e-04\n",
      "Epoch 14900 | Total: 5.494e-04 | Int: 3.017e-04 | BC: 2.477e-04 | L2: 3.010e-03 | Energy: 1.876e-02 | LR: 2.50e-04\n",
      "Epoch 15000 | Total: 5.438e-04 | Int: 2.985e-04 | BC: 2.454e-04 | L2: 4.831e-03 | Energy: 2.046e-02 | LR: 2.50e-04\n",
      "Epoch 15100 | Total: 5.382e-04 | Int: 2.951e-04 | BC: 2.431e-04 | L2: 7.091e-03 | Energy: 2.261e-02 | LR: 2.50e-04\n",
      "Epoch 15200 | Total: 5.325e-04 | Int: 2.917e-04 | BC: 2.408e-04 | L2: 9.512e-03 | Energy: 2.493e-02 | LR: 2.50e-04\n",
      "Epoch 15300 | Total: 5.268e-04 | Int: 2.882e-04 | BC: 2.386e-04 | L2: 1.204e-02 | Energy: 2.734e-02 | LR: 2.50e-04\n",
      "Epoch 15400 | Total: 5.210e-04 | Int: 2.847e-04 | BC: 2.364e-04 | L2: 1.465e-02 | Energy: 2.984e-02 | LR: 2.50e-04\n",
      "Epoch 15500 | Total: 5.151e-04 | Int: 2.810e-04 | BC: 2.341e-04 | L2: 1.734e-02 | Energy: 3.243e-02 | LR: 2.50e-04\n",
      "Epoch 15600 | Total: 5.092e-04 | Int: 2.773e-04 | BC: 2.318e-04 | L2: 2.012e-02 | Energy: 3.510e-02 | LR: 2.50e-04\n",
      "Epoch 15700 | Total: 5.550e-04 | Int: 3.250e-04 | BC: 2.300e-04 | L2: 2.301e-02 | Energy: 3.786e-02 | LR: 2.50e-04\n",
      "Epoch 15800 | Total: 5.427e-04 | Int: 3.127e-04 | BC: 2.300e-04 | L2: 2.359e-02 | Energy: 3.848e-02 | LR: 2.50e-04\n",
      "Epoch 15900 | Total: 6.974e-03 | Int: 6.448e-03 | BC: 5.254e-04 | L2: 2.749e-02 | Energy: 4.155e-02 | LR: 2.50e-04\n",
      "Epoch 16000 | Total: 5.211e-04 | Int: 2.930e-04 | BC: 2.281e-04 | L2: 2.878e-02 | Energy: 4.350e-02 | LR: 2.50e-04\n",
      "Epoch 16100 | Total: 4.948e-04 | Int: 2.678e-04 | BC: 2.270e-04 | L2: 3.112e-02 | Energy: 4.575e-02 | LR: 2.50e-04\n",
      "Epoch 16200 | Total: 4.849e-04 | Int: 2.595e-04 | BC: 2.253e-04 | L2: 3.311e-02 | Energy: 4.772e-02 | LR: 2.50e-04\n",
      "Epoch 16300 | Total: 4.855e-04 | Int: 2.612e-04 | BC: 2.242e-04 | L2: 3.542e-02 | Energy: 4.994e-02 | LR: 2.50e-04\n",
      "Epoch 16400 | Total: 4.914e-04 | Int: 2.703e-04 | BC: 2.211e-04 | L2: 3.800e-02 | Energy: 5.251e-02 | LR: 2.50e-04\n",
      "Epoch 16500 | Total: 6.458e-04 | Int: 4.219e-04 | BC: 2.239e-04 | L2: 3.932e-02 | Energy: 5.375e-02 | LR: 2.50e-04\n",
      "Epoch 16600 | Total: 5.575e-04 | Int: 3.368e-04 | BC: 2.207e-04 | L2: 4.261e-02 | Energy: 5.697e-02 | LR: 2.50e-04\n",
      "Epoch 16700 | Total: 5.870e-04 | Int: 3.648e-04 | BC: 2.222e-04 | L2: 4.409e-02 | Energy: 5.835e-02 | LR: 2.50e-04\n",
      "Epoch 16800 | Total: 5.099e-04 | Int: 2.917e-04 | BC: 2.183e-04 | L2: 4.600e-02 | Energy: 6.025e-02 | LR: 2.50e-04\n",
      "Epoch 16900 | Total: 4.590e-04 | Int: 2.425e-04 | BC: 2.165e-04 | L2: 4.857e-02 | Energy: 6.277e-02 | LR: 2.50e-04\n",
      "Epoch 17000 | Total: 7.448e-04 | Int: 5.231e-04 | BC: 2.217e-04 | L2: 5.117e-02 | Energy: 6.522e-02 | LR: 2.50e-04\n",
      "Epoch 17100 | Total: 6.343e-04 | Int: 4.157e-04 | BC: 2.186e-04 | L2: 5.284e-02 | Energy: 6.712e-02 | LR: 2.50e-04\n",
      "Epoch 17200 | Total: 1.748e-02 | Int: 1.713e-02 | BC: 3.560e-04 | L2: 5.511e-02 | Energy: 6.931e-02 | LR: 2.50e-04\n",
      "Epoch 17300 | Total: 4.438e-04 | Int: 2.326e-04 | BC: 2.112e-04 | L2: 5.750e-02 | Energy: 7.149e-02 | LR: 2.50e-04\n",
      "Epoch 17400 | Total: 8.937e-04 | Int: 6.565e-04 | BC: 2.372e-04 | L2: 5.793e-02 | Energy: 7.176e-02 | LR: 2.50e-04\n",
      "Epoch 17500 | Total: 5.500e-04 | Int: 3.387e-04 | BC: 2.114e-04 | L2: 6.082e-02 | Energy: 7.482e-02 | LR: 2.50e-04\n",
      "Epoch 17600 | Total: 5.707e-04 | Int: 3.571e-04 | BC: 2.137e-04 | L2: 6.311e-02 | Energy: 7.689e-02 | LR: 2.50e-04\n",
      "Epoch 17700 | Total: 1.593e-03 | Int: 1.369e-03 | BC: 2.238e-04 | L2: 6.576e-02 | Energy: 7.935e-02 | LR: 2.50e-04\n",
      "Epoch 17800 | Total: 5.209e-04 | Int: 3.127e-04 | BC: 2.081e-04 | L2: 6.728e-02 | Energy: 8.109e-02 | LR: 2.50e-04\n",
      "Epoch 17900 | Total: 6.521e-04 | Int: 4.448e-04 | BC: 2.073e-04 | L2: 6.972e-02 | Energy: 8.339e-02 | LR: 2.50e-04\n",
      "Epoch 18000 | Total: 4.224e-04 | Int: 2.182e-04 | BC: 2.042e-04 | L2: 7.142e-02 | Energy: 8.503e-02 | LR: 2.50e-04\n",
      "Epoch 18100 | Total: 4.586e-04 | Int: 2.522e-04 | BC: 2.064e-04 | L2: 7.197e-02 | Energy: 8.556e-02 | LR: 2.50e-04\n",
      "Epoch 18200 | Total: 4.338e-04 | Int: 2.303e-04 | BC: 2.034e-04 | L2: 7.456e-02 | Energy: 8.812e-02 | LR: 2.50e-04\n",
      "Epoch 18300 | Total: 4.181e-04 | Int: 2.166e-04 | BC: 2.015e-04 | L2: 7.700e-02 | Energy: 9.050e-02 | LR: 2.50e-04\n",
      "Epoch 18400 | Total: 4.476e-03 | Int: 4.236e-03 | BC: 2.393e-04 | L2: 7.830e-02 | Energy: 9.192e-02 | LR: 2.50e-04\n",
      "Epoch 18500 | Total: 4.787e-04 | Int: 2.726e-04 | BC: 2.061e-04 | L2: 8.031e-02 | Energy: 9.362e-02 | LR: 2.50e-04\n",
      "Epoch 18600 | Total: 4.612e-04 | Int: 2.610e-04 | BC: 2.003e-04 | L2: 8.263e-02 | Energy: 9.600e-02 | LR: 2.50e-04\n",
      "Epoch 18700 | Total: 2.102e-03 | Int: 1.886e-03 | BC: 2.163e-04 | L2: 8.356e-02 | Energy: 9.693e-02 | LR: 2.50e-04\n",
      "Epoch 18800 | Total: 1.672e-03 | Int: 1.406e-03 | BC: 2.664e-04 | L2: 8.629e-02 | Energy: 9.925e-02 | LR: 2.50e-04\n",
      "Epoch 18900 | Total: 4.620e-04 | Int: 2.628e-04 | BC: 1.992e-04 | L2: 8.803e-02 | Energy: 1.012e-01 | LR: 2.50e-04\n",
      "Epoch 19000 | Total: 8.416e-04 | Int: 6.297e-04 | BC: 2.119e-04 | L2: 8.916e-02 | Energy: 1.022e-01 | LR: 2.50e-04\n",
      "Epoch 19100 | Total: 4.228e-04 | Int: 2.268e-04 | BC: 1.959e-04 | L2: 9.124e-02 | Energy: 1.044e-01 | LR: 2.50e-04\n",
      "Epoch 19200 | Total: 4.065e-04 | Int: 2.125e-04 | BC: 1.940e-04 | L2: 9.290e-02 | Energy: 1.061e-01 | LR: 2.50e-04\n",
      "Epoch 19300 | Total: 6.920e-04 | Int: 4.891e-04 | BC: 2.030e-04 | L2: 9.541e-02 | Energy: 1.084e-01 | LR: 2.50e-04\n",
      "Epoch 19400 | Total: 1.603e-03 | Int: 1.395e-03 | BC: 2.080e-04 | L2: 9.509e-02 | Energy: 1.082e-01 | LR: 2.50e-04\n",
      "Epoch 19500 | Total: 3.816e-04 | Int: 1.910e-04 | BC: 1.906e-04 | L2: 9.882e-02 | Energy: 1.118e-01 | LR: 2.50e-04\n",
      "Epoch 19600 | Total: 4.316e-04 | Int: 2.410e-04 | BC: 1.906e-04 | L2: 9.976e-02 | Energy: 1.128e-01 | LR: 2.50e-04\n",
      "Epoch 19700 | Total: 3.788e-04 | Int: 1.884e-04 | BC: 1.903e-04 | L2: 1.013e-01 | Energy: 1.143e-01 | LR: 2.50e-04\n",
      "Epoch 19800 | Total: 3.825e-04 | Int: 1.926e-04 | BC: 1.899e-04 | L2: 1.034e-01 | Energy: 1.163e-01 | LR: 2.50e-04\n",
      "Epoch 19900 | Total: 3.824e-04 | Int: 1.931e-04 | BC: 1.893e-04 | L2: 1.042e-01 | Energy: 1.171e-01 | LR: 2.50e-04\n",
      "\n",
      "Training time (Example 1): 1407.34 s\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS - EXAMPLE 1 (P4)\n",
      "============================================================\n",
      "L2 Error           : 1.060240e-01\n",
      "Energy Error       : 1.185807e-01\n",
      "Relative L2 Error  : 4.211442e+00\n",
      "Relative H1 Error  : 8.604641e-01\n",
      "All plots & model saved in: pinn_biharmonic_results_pytorch_corrected\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================= EXAMPLE 1: P4 (Cahn–Hilliard) =========================\n",
    "# u(x,y) = (1/(2π²)) sin(πx) sin(πy)\n",
    "# Domain: Ω = (0,1)²\n",
    "# PDE: Δ²u = f, with P4 boundary conditions:\n",
    "#       ∂u/∂n = g1,  ∂(Δu)/∂n = g2 on ∂Ω\n",
    "# Network: [2, 84, 84, 84, 84, 1]\n",
    "# Training: 20000 epochs, 20000 interior pts, 6000 boundary pts\n",
    "# Uses Adam + ReduceLROnPlateau LR scheduler.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Setup\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "results_dir = \"pinn_biharmonic_results_pytorch_corrected\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Neural Network\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class BiharmonicPINN(nn.Module):\n",
    "    def __init__(self, layers, activation=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.linears.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.xavier_normal_(linear.weight)\n",
    "            nn.init.constant_(linear.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            if i < len(self.linears) - 1:\n",
    "                x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Differential operators (up to 4th order)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def compute_derivatives(u, x, y):\n",
    "    \"\"\"\n",
    "    Compute derivatives up to 4th order using autograd.\n",
    "\n",
    "    Returns:\n",
    "        u, u_x, u_y, u_xx, u_yy, u_xy,\n",
    "        u_xxx, u_xxy, u_xyy, u_yyy,\n",
    "        u_xxxx, u_xxyy, u_yyyy\n",
    "    \"\"\"\n",
    "    # first derivatives\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u),\n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # second derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    u_xy = torch.autograd.grad(u_x, y, grad_outputs=torch.ones_like(u_x),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # third derivatives\n",
    "    u_xxx = torch.autograd.grad(u_xx, x, grad_outputs=torch.ones_like(u_xx),\n",
    "                                create_graph=True, retain_graph=True)[0]\n",
    "    u_xxy = torch.autograd.grad(u_xx, y, grad_outputs=torch.ones_like(u_xx),\n",
    "                                create_graph=True, retain_graph=True)[0]\n",
    "    u_xyy = torch.autograd.grad(u_xy, y, grad_outputs=torch.ones_like(u_xy),\n",
    "                                create_graph=True, retain_graph=True)[0]\n",
    "    u_yyy = torch.autograd.grad(u_yy, y, grad_outputs=torch.ones_like(u_yy),\n",
    "                                create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # fourth derivatives\n",
    "    u_xxxx = torch.autograd.grad(u_xxx, x, grad_outputs=torch.ones_like(u_xxx),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "    u_xxyy = torch.autograd.grad(u_xxy, y, grad_outputs=torch.ones_like(u_xxy),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "    u_yyyy = torch.autograd.grad(u_yyy, y, grad_outputs=torch.ones_like(u_yyy),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    return (u, u_x, u_y, u_xx, u_yy, u_xy,\n",
    "            u_xxx, u_xxy, u_xyy, u_yyy,\n",
    "            u_xxxx, u_xxyy, u_yyyy)\n",
    "\n",
    "def compute_biharmonic(u_xxxx, u_xxyy, u_yyyy):\n",
    "    \"\"\"Δ²u = u_xxxx + 2 u_xxyy + u_yyyy\"\"\"\n",
    "    return u_xxxx + 2.0 * u_xxyy + u_yyyy\n",
    "\n",
    "def compute_normals(x, y):\n",
    "    \"\"\"\n",
    "    Outward unit normal on ∂Ω for Ω = (0,1)².\n",
    "\n",
    "        left   (x=0): n = (-1, 0)\n",
    "        right  (x=1): n = ( 1, 0)\n",
    "        bottom (y=0): n = ( 0,-1)\n",
    "        top    (y=1): n = ( 0, 1)\n",
    "    \"\"\"\n",
    "    n_x = torch.zeros_like(x)\n",
    "    n_y = torch.zeros_like(y)\n",
    "    eps = 1e-6\n",
    "\n",
    "    left   = x <= eps\n",
    "    right  = x >= 1.0 - eps\n",
    "    bottom = y <= eps\n",
    "    top    = y >= 1.0 - eps\n",
    "\n",
    "    n_x[left] = -1.0\n",
    "    n_x[right] = 1.0\n",
    "    n_y[bottom] = -1.0\n",
    "    n_y[top] = 1.0\n",
    "\n",
    "    return n_x, n_y\n",
    "\n",
    "def compute_normal_derivatives(x, y,\n",
    "                               u_x, u_y,\n",
    "                               u_xx, u_yy, u_xy,\n",
    "                               u_xxx, u_xxy, u_xyy, u_yyy):\n",
    "    \"\"\"\n",
    "    P4 boundary operators:\n",
    "        ∂u/∂n      = n · ∇u\n",
    "        ∂(Δu)/∂n   = n · ∇(Δu)\n",
    "    \"\"\"\n",
    "    n_x, n_y = compute_normals(x, y)\n",
    "\n",
    "    # ∂u/∂n\n",
    "    u_n = n_x * u_x + n_y * u_y\n",
    "\n",
    "    # Δu = u_xx + u_yy\n",
    "    # ∂(Δu)/∂x = u_xxx + u_xyy\n",
    "    # ∂(Δu)/∂y = u_xxy + u_yyy\n",
    "    lap_x = u_xxx + u_xyy\n",
    "    lap_y = u_xxy + u_yyy\n",
    "    lap_n = n_x * lap_x + n_y * lap_y\n",
    "\n",
    "    return u_n, lap_n\n",
    "\n",
    "def compute_errors(u_pred, u_exact, x, y):\n",
    "    \"\"\"\n",
    "    L2 and \"energy\" (H1-like) errors:\n",
    "        L2  = ||uθ - u||_L2\n",
    "        H1  ≈ L2(uθ - u) + L2(∇uθ - ∇u)\n",
    "    plus relative versions.\n",
    "    \"\"\"\n",
    "    diff = u_pred - u_exact\n",
    "    l2_error = torch.sqrt(torch.mean(diff ** 2))\n",
    "\n",
    "    # gradients\n",
    "    u_pred_x = torch.autograd.grad(u_pred, x, grad_outputs=torch.ones_like(u_pred),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "    u_pred_y = torch.autograd.grad(u_pred, y, grad_outputs=torch.ones_like(u_pred),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_x = torch.autograd.grad(u_exact, x, grad_outputs=torch.ones_like(u_exact),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_y = torch.autograd.grad(u_exact, y, grad_outputs=torch.ones_like(u_exact),\n",
    "                                    create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    grad_diff_sq = (u_pred_x - u_exact_x) ** 2 + (u_pred_y - u_exact_y) ** 2\n",
    "    grad_error = torch.sqrt(torch.mean(grad_diff_sq))\n",
    "\n",
    "    energy_error = l2_error + grad_error\n",
    "\n",
    "    # exact norms\n",
    "    l2_norm_exact = torch.sqrt(torch.mean(u_exact ** 2))\n",
    "    grad_exact_sq = u_exact_x ** 2 + u_exact_y ** 2\n",
    "    grad_norm_exact = torch.sqrt(torch.mean(grad_exact_sq))\n",
    "    energy_norm_exact = l2_norm_exact + grad_norm_exact\n",
    "\n",
    "    l2_rel = l2_error / (l2_norm_exact + 1e-12)\n",
    "    energy_rel = energy_error / (energy_norm_exact + 1e-12)\n",
    "\n",
    "    return l2_error, energy_error, l2_rel, energy_rel\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example 1: exact solution and source term\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def exact_solution1(x, y):\n",
    "    return (1.0 / (2.0 * np.pi ** 2)) * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "def source_term1(x, y):\n",
    "    # Derived from u: Δ²u = 2π² sin(πx) sin(πy)\n",
    "    return (2.0 * np.pi ** 2) * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training data\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "N_int = 20000   # interior points\n",
    "N_bc  = 6000    # boundary points\n",
    "\n",
    "# interior in (0,1)²\n",
    "x_int = torch.rand((N_int, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_int = torch.rand((N_int, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# boundary points – 4 sides\n",
    "N_bc_side = N_bc // 4\n",
    "\n",
    "# bottom: y = 0\n",
    "x_bc_bottom = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_bottom = torch.zeros_like(x_bc_bottom, requires_grad=True)\n",
    "\n",
    "# top: y = 1\n",
    "x_bc_top = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_top = torch.ones_like(x_bc_top, requires_grad=True)\n",
    "\n",
    "# left: x = 0\n",
    "x_bc_left = torch.zeros((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_left = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# right: x = 1\n",
    "x_bc_right = torch.ones((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_right = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "x_bc = torch.cat([x_bc_bottom, x_bc_top, x_bc_left, x_bc_right], dim=0)\n",
    "y_bc = torch.cat([y_bc_bottom, y_bc_top, y_bc_left, y_bc_right], dim=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Model, optimizer, scheduler, and test points\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "layers = [2, 84, 84, 84, 84, 1]\n",
    "pinn = BiharmonicPINN(layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)\n",
    "\n",
    "# LR scheduler: reduce LR by factor 0.5 when loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=500)\n",
    "\n",
    "epochs = 20000\n",
    "print_interval = 100\n",
    "\n",
    "# histories (one entry every 100 epochs)\n",
    "loss_history = []\n",
    "int_loss_history = []\n",
    "bc_loss_history = []\n",
    "l2_error_history = []\n",
    "energy_error_history = []\n",
    "\n",
    "# fixed test points for monitoring errors\n",
    "x_test = torch.rand((1000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_test = torch.rand((1000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "def train_step():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ---- interior (PDE: Δ²u = f) ----\n",
    "    X_int = torch.cat([x_int, y_int], dim=1)\n",
    "    u_int = pinn(X_int)\n",
    "\n",
    "    (u_int,\n",
    "     u_x_int, u_y_int,\n",
    "     u_xx_int, u_yy_int, u_xy_int,\n",
    "     u_xxx_int, u_xxy_int, u_xyy_int, u_yyy_int,\n",
    "     u_xxxx_int, u_xxyy_int, u_yyyy_int) = compute_derivatives(u_int, x_int, y_int)\n",
    "\n",
    "    bih_int = compute_biharmonic(u_xxxx_int, u_xxyy_int, u_yyyy_int)\n",
    "    f_int = source_term1(x_int, y_int)\n",
    "\n",
    "    loss_int = torch.mean((bih_int - f_int) ** 2)\n",
    "\n",
    "    # ---- boundary (P4: ∂u/∂n = g1, ∂Δu/∂n = g2) ----\n",
    "    X_bc = torch.cat([x_bc, y_bc], dim=1)\n",
    "    u_bc = pinn(X_bc)\n",
    "\n",
    "    (u_bc,\n",
    "     u_x_bc, u_y_bc,\n",
    "     u_xx_bc, u_yy_bc, u_xy_bc,\n",
    "     u_xxx_bc, u_xxy_bc, u_xyy_bc, u_yyy_bc,\n",
    "     _, _, _) = compute_derivatives(u_bc, x_bc, y_bc)\n",
    "\n",
    "    # predicted BC operators\n",
    "    u_n_bc, lap_n_bc = compute_normal_derivatives(\n",
    "        x_bc, y_bc,\n",
    "        u_x_bc, u_y_bc,\n",
    "        u_xx_bc, u_yy_bc, u_xy_bc,\n",
    "        u_xxx_bc, u_xxy_bc, u_xyy_bc, u_yyy_bc\n",
    "    )\n",
    "\n",
    "    # exact BC from exact u\n",
    "    u_exact_bc = exact_solution1(x_bc, y_bc)\n",
    "\n",
    "    u_exact_x_bc = torch.autograd.grad(u_exact_bc, x_bc,\n",
    "                                       grad_outputs=torch.ones_like(u_exact_bc),\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_y_bc = torch.autograd.grad(u_exact_bc, y_bc,\n",
    "                                       grad_outputs=torch.ones_like(u_exact_bc),\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    n_x_bc, n_y_bc = compute_normals(x_bc, y_bc)\n",
    "    u_n_exact_bc = n_x_bc * u_exact_x_bc + n_y_bc * u_exact_y_bc\n",
    "\n",
    "    u_exact_xx_bc = torch.autograd.grad(u_exact_x_bc, x_bc,\n",
    "                                        grad_outputs=torch.ones_like(u_exact_x_bc),\n",
    "                                        create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_yy_bc = torch.autograd.grad(u_exact_y_bc, y_bc,\n",
    "                                        grad_outputs=torch.ones_like(u_exact_y_bc),\n",
    "                                        create_graph=True, retain_graph=True)[0]\n",
    "    lap_exact_bc = u_exact_xx_bc + u_exact_yy_bc\n",
    "\n",
    "    lap_x_exact = torch.autograd.grad(lap_exact_bc, x_bc,\n",
    "                                      grad_outputs=torch.ones_like(lap_exact_bc),\n",
    "                                      create_graph=True, retain_graph=True)[0]\n",
    "    lap_y_exact = torch.autograd.grad(lap_exact_bc, y_bc,\n",
    "                                      grad_outputs=torch.ones_like(lap_exact_bc),\n",
    "                                      create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    lap_n_exact_bc = n_x_bc * lap_x_exact + n_y_bc * lap_y_exact\n",
    "\n",
    "    loss_bc = torch.mean((u_n_bc - u_n_exact_bc) ** 2) + \\\n",
    "              torch.mean((lap_n_bc - lap_n_exact_bc) ** 2)\n",
    "\n",
    "    # total loss\n",
    "    total_loss = loss_int + loss_bc\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss, loss_int, loss_bc\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Example 1: u = (1/(2π²)) sin(πx) sin(πy) with P4 BCs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, loss_int, loss_bc = train_step()\n",
    "\n",
    "    # Learning rate scheduler (use scalar loss)\n",
    "    scheduler.step(total_loss.item())\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        # compute monitoring errors (need gradients -> no torch.no_grad())\n",
    "        x_eval = x_test\n",
    "        y_eval = y_test\n",
    "        X_eval = torch.cat([x_eval, y_eval], dim=1)\n",
    "\n",
    "        u_pred = pinn(X_eval)\n",
    "        u_exact = exact_solution1(x_eval, y_eval)\n",
    "\n",
    "        l2_err, en_err, l2_rel, en_rel = compute_errors(\n",
    "            u_pred, u_exact, x_eval, y_eval\n",
    "        )\n",
    "\n",
    "        loss_history.append(total_loss.item())\n",
    "        int_loss_history.append(loss_int.item())\n",
    "        bc_loss_history.append(loss_bc.item())\n",
    "        l2_error_history.append(l2_err.item())\n",
    "        energy_error_history.append(en_err.item())\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(f\"Epoch {epoch:5d} | \"\n",
    "              f\"Total: {total_loss.item():.3e} | \"\n",
    "              f\"Int: {loss_int.item():.3e} | \"\n",
    "              f\"BC: {loss_bc.item():.3e} | \"\n",
    "              f\"L2: {l2_err.item():.3e} | \"\n",
    "              f\"Energy: {en_err.item():.3e} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining time (Example 1): {training_time:.2f} s\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Post-processing: plots and model save\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# grid for visualization (no gradients needed here)\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "y_plot = np.linspace(0, 1, 100)\n",
    "X_plot, Y_plot = np.meshgrid(x_plot, y_plot)\n",
    "X_flat = X_plot.flatten().reshape(-1, 1)\n",
    "Y_flat = Y_plot.flatten().reshape(-1, 1)\n",
    "XY_plot = torch.tensor(np.hstack([X_flat, Y_flat]), dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred_plot = pinn(XY_plot).cpu().numpy().reshape(100, 100)\n",
    "\n",
    "u_exact_plot = exact_solution1(\n",
    "    torch.tensor(X_flat, dtype=torch.float32).to(device),\n",
    "    torch.tensor(Y_flat, dtype=torch.float32).to(device)\n",
    ").detach().cpu().numpy().reshape(100, 100)\n",
    "\n",
    "error_plot = np.abs(u_pred_plot - u_exact_plot)\n",
    "\n",
    "# 1) loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(loss_history, label=\"Total Loss\", linewidth=2)\n",
    "plt.semilogy(int_loss_history, label=\"Interior Loss\", linewidth=2)\n",
    "plt.semilogy(bc_loss_history, label=\"Boundary Loss\", linewidth=2)\n",
    "plt.xlabel(\"Index (every 100 epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Example 1: Training Loss History (P4)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(results_dir, \"example1_loss_history.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 2) error history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(l2_error_history, label=\"L2 Error\", linewidth=2)\n",
    "plt.semilogy(energy_error_history, label=\"Energy Error\", linewidth=2)\n",
    "plt.xlabel(\"Index (every 100 epochs)\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Example 1: Error History (P4)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(results_dir, \"example1_error_history.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 3) predicted solution\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, u_pred_plot, levels=50, cmap=\"viridis\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: PINN Predicted Solution (P4)\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_predicted_solution.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 4) exact solution\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, u_exact_plot, levels=50, cmap=\"viridis\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: Exact Solution\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_exact_solution.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 5) absolute error\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, error_plot, levels=50, cmap=\"hot\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: Absolute Error |u - uθ|\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_absolute_error.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# save model\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": pinn.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"training_time\": training_time,\n",
    "    },\n",
    "    os.path.join(results_dir, \"example1_model.pth\"),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Final error evaluation (NEEDS gradients -> no torch.no_grad)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "x_eval = torch.rand((2000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_eval = torch.rand((2000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "X_eval = torch.cat([x_eval, y_eval], dim=1)\n",
    "\n",
    "u_pred_final = pinn(X_eval)\n",
    "u_exact_final = exact_solution1(x_eval, y_eval)\n",
    "\n",
    "l2_error_final, energy_error_final, l2_rel_final, energy_rel_final = compute_errors(\n",
    "    u_pred_final, u_exact_final, x_eval, y_eval\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS - EXAMPLE 1 (P4)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"L2 Error           : {l2_error_final.item():.6e}\")\n",
    "print(f\"Energy Error       : {energy_error_final.item():.6e}\")\n",
    "print(f\"Relative L2 Error  : {l2_rel_final.item():.6e}\")\n",
    "print(f\"Relative H1 Error  : {energy_rel_final.item():.6e}\")\n",
    "print(f\"All plots & model saved in: {results_dir}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aebb24-dbe9-484a-bd7a-762f2c97a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "Training Example 1: u = (1/(2π²)) sin(πx) sin(πy) with P4 BCs\n",
      "================================================================================\n",
      "Epoch     0 | Total: 5.899e+02 | Int: 9.749e+01 | BC: 4.924e+00 | L2: 1.701e-02 | Energy: 1.315e-01 | LR: 1.00e-03\n",
      "Epoch   100 | Total: 5.437e+01 | Int: 2.273e+01 | BC: 3.163e-01 | L2: 3.864e-01 | Energy: 5.067e-01 | LR: 1.00e-03\n",
      "Epoch   200 | Total: 2.055e+01 | Int: 8.719e+00 | BC: 1.184e-01 | L2: 2.574e-01 | Energy: 3.155e-01 | LR: 1.00e-03\n",
      "Epoch   300 | Total: 1.476e+01 | Int: 7.224e+00 | BC: 7.531e-02 | L2: 1.940e-01 | Energy: 2.262e-01 | LR: 1.00e-03\n",
      "Epoch   400 | Total: 8.733e+00 | Int: 5.153e+00 | BC: 3.579e-02 | L2: 1.105e-01 | Energy: 1.370e-01 | LR: 1.00e-03\n",
      "Epoch   500 | Total: 4.212e-01 | Int: 2.438e-01 | BC: 1.774e-03 | L2: 2.557e-02 | Energy: 4.247e-02 | LR: 1.00e-03\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Setup\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "results_dir = \"pinn_biharmonic_results_pytorch_corrected\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Neural Network\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "class BiharmonicPINN(nn.Module):\n",
    "    # --- CORRECTED: Switched to SiLU for more stable high-order gradients ---\n",
    "    def __init__(self, layers, activation=nn.SiLU()):\n",
    "    # --- END CORRECTION ---\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.linears.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.xavier_normal_(linear.weight)\n",
    "            nn.init.constant_(linear.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            if i < len(self.linears) - 1:\n",
    "                x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Differential operators (up to 4th order)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def compute_derivatives(u, x, y):\n",
    "    \"\"\"\n",
    "    Compute derivatives up to 4th order using autograd.\n",
    "    \"\"\"\n",
    "    # first derivatives\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                             create_graph=True, retain_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u),\n",
    "                             create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # second derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x),\n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y),\n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "    u_xy = torch.autograd.grad(u_x, y, grad_outputs=torch.ones_like(u_x),\n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # third derivatives\n",
    "    u_xxx = torch.autograd.grad(u_xx, x, grad_outputs=torch.ones_like(u_xx),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    u_xxy = torch.autograd.grad(u_xx, y, grad_outputs=torch.ones_like(u_xx),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    u_xyy = torch.autograd.grad(u_xy, y, grad_outputs=torch.ones_like(u_xy),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    u_yyy = torch.autograd.grad(u_yy, y, grad_outputs=torch.ones_like(u_yy),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # fourth derivatives\n",
    "    u_xxxx = torch.autograd.grad(u_xxx, x, grad_outputs=torch.ones_like(u_xxx),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    u_xxyy = torch.autograd.grad(u_xxy, y, grad_outputs=torch.ones_like(u_xxy),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    u_yyyy = torch.autograd.grad(u_yyy, y, grad_outputs=torch.ones_like(u_yyy),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    return (u, u_x, u_y, u_xx, u_yy, u_xy,\n",
    "            u_xxx, u_xxy, u_xyy, u_yyy,\n",
    "            u_xxxx, u_xxyy, u_yyyy)\n",
    "\n",
    "def compute_biharmonic(u_xxxx, u_xxyy, u_yyyy):\n",
    "    \"\"\"Δ²u = u_xxxx + 2 u_xxyy + u_yyyy\"\"\"\n",
    "    return u_xxxx + 2.0 * u_xxyy + u_yyyy\n",
    "\n",
    "def compute_normals(x, y):\n",
    "    \"\"\"\n",
    "    Outward unit normal on ∂Ω for Ω = (0,1)².\n",
    "    \"\"\"\n",
    "    n_x = torch.zeros_like(x)\n",
    "    n_y = torch.zeros_like(y)\n",
    "    eps = 1e-6\n",
    "\n",
    "    left   = x <= eps\n",
    "    right  = x >= 1.0 - eps\n",
    "    bottom = y <= eps\n",
    "    top    = y >= 1.0 - eps\n",
    "\n",
    "    n_x[left] = -1.0\n",
    "    n_x[right] = 1.0\n",
    "    n_y[bottom] = -1.0\n",
    "    n_y[top] = 1.0\n",
    "\n",
    "    return n_x, n_y\n",
    "\n",
    "def compute_normal_derivatives(x, y,\n",
    "                               u_x, u_y,\n",
    "                               u_xx, u_yy, u_xy,\n",
    "                               u_xxx, u_xxy, u_xyy, u_yyy):\n",
    "    \"\"\"\n",
    "    P4 boundary operators:\n",
    "        ∂u/∂n      = n · ∇u\n",
    "        ∂(Δu)/∂n   = n · ∇(Δu)\n",
    "    \"\"\"\n",
    "    n_x, n_y = compute_normals(x, y)\n",
    "\n",
    "    # ∂u/∂n\n",
    "    u_n = n_x * u_x + n_y * u_y\n",
    "\n",
    "    # ∂(Δu)/∂n\n",
    "    lap_x = u_xxx + u_xyy\n",
    "    lap_y = u_xxy + u_yyy\n",
    "    lap_n = n_x * lap_x + n_y * lap_y\n",
    "\n",
    "    return u_n, lap_n\n",
    "\n",
    "def compute_errors(u_pred, u_exact, x, y):\n",
    "    \"\"\"\n",
    "    L2 and \"energy\" (H1-like) errors.\n",
    "    \"\"\"\n",
    "    diff = u_pred - u_exact\n",
    "    l2_error = torch.sqrt(torch.mean(diff ** 2))\n",
    "\n",
    "    # gradients\n",
    "    u_pred_x = torch.autograd.grad(u_pred, x, grad_outputs=torch.ones_like(u_pred),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    u_pred_y = torch.autograd.grad(u_pred, y, grad_outputs=torch.ones_like(u_pred),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_x = torch.autograd.grad(u_exact, x, grad_outputs=torch.ones_like(u_exact),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_y = torch.autograd.grad(u_exact, y, grad_outputs=torch.ones_like(u_exact),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    grad_diff_sq = (u_pred_x - u_exact_x) ** 2 + (u_pred_y - u_exact_y) ** 2\n",
    "    grad_error = torch.sqrt(torch.mean(grad_diff_sq))\n",
    "\n",
    "    energy_error = l2_error + grad_error\n",
    "\n",
    "    # exact norms\n",
    "    l2_norm_exact = torch.sqrt(torch.mean(u_exact ** 2))\n",
    "    grad_exact_sq = u_exact_x ** 2 + u_exact_y ** 2\n",
    "    grad_norm_exact = torch.sqrt(torch.mean(grad_exact_sq))\n",
    "    energy_norm_exact = l2_norm_exact + grad_norm_exact\n",
    "\n",
    "    l2_rel = l2_error / (l2_norm_exact + 1e-12)\n",
    "    energy_rel = energy_error / (energy_norm_exact + 1e-12)\n",
    "\n",
    "    return l2_error, energy_error, l2_rel, energy_rel\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example 1: exact solution and source term\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def exact_solution1(x, y):\n",
    "    return (1.0 / (2.0 * np.pi ** 2)) * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "def source_term1(x, y):\n",
    "    # Derived from u: Δ²u = 2π² sin(πx) sin(πy)\n",
    "    return (2.0 * np.pi ** 2) * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training data\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "N_int = 20000  # interior points\n",
    "N_bc  = 6000   # boundary points\n",
    "\n",
    "# interior in (0,1)²\n",
    "x_int = torch.rand((N_int, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_int = torch.rand((N_int, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# boundary points – 4 sides\n",
    "N_bc_side = N_bc // 4\n",
    "\n",
    "# bottom: y = 0\n",
    "x_bc_bottom = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_bottom = torch.zeros_like(x_bc_bottom, requires_grad=True)\n",
    "\n",
    "# top: y = 1\n",
    "x_bc_top = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_top = torch.ones_like(x_bc_top, requires_grad=True)\n",
    "\n",
    "# left: x = 0\n",
    "x_bc_left = torch.zeros((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_left = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# right: x = 1\n",
    "x_bc_right = torch.ones((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_bc_right = torch.rand((N_bc_side, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "x_bc = torch.cat([x_bc_bottom, x_bc_top, x_bc_left, x_bc_right], dim=0)\n",
    "y_bc = torch.cat([y_bc_bottom, y_bc_top, y_bc_left, y_bc_right], dim=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Model, optimizer, scheduler, and test points\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "layers = [2, 84, 84, 84, 84, 1]\n",
    "pinn = BiharmonicPINN(layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)\n",
    "\n",
    "# LR scheduler: reduce LR by factor 0.5 when loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=500)\n",
    "\n",
    "epochs = 20000\n",
    "print_interval = 100\n",
    "\n",
    "# histories (one entry every 100 epochs)\n",
    "loss_history = []\n",
    "int_loss_history = []\n",
    "bc_loss_history = []\n",
    "l2_error_history = []\n",
    "energy_error_history = []\n",
    "\n",
    "# fixed test points for monitoring errors\n",
    "x_test = torch.rand((1000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_test = torch.rand((1000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "def train_step():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ---- interior (PDE: Δ²u = f) ----\n",
    "    X_int = torch.cat([x_int, y_int], dim=1)\n",
    "    u_int = pinn(X_int)\n",
    "\n",
    "    (u_int,\n",
    "     u_x_int, u_y_int,\n",
    "     u_xx_int, u_yy_int, u_xy_int,\n",
    "     u_xxx_int, u_xxy_int, u_xyy_int, u_yyy_int,\n",
    "     u_xxxx_int, u_xxyy_int, u_yyyy_int) = compute_derivatives(u_int, x_int, y_int)\n",
    "\n",
    "    bih_int = compute_biharmonic(u_xxxx_int, u_xxyy_int, u_yyyy_int)\n",
    "    f_int = source_term1(x_int, y_int)\n",
    "\n",
    "    loss_int = torch.mean((bih_int - f_int) ** 2)\n",
    "\n",
    "    # ---- boundary (P4: ∂u/∂n = g1, ∂Δu/∂n = g2) ----\n",
    "    X_bc = torch.cat([x_bc, y_bc], dim=1)\n",
    "    u_bc = pinn(X_bc)\n",
    "\n",
    "    (u_bc,\n",
    "     u_x_bc, u_y_bc,\n",
    "     u_xx_bc, u_yy_bc, u_xy_bc,\n",
    "     u_xxx_bc, u_xxy_bc, u_xyy_bc, u_yyy_bc,\n",
    "     _, _, _) = compute_derivatives(u_bc, x_bc, y_bc)\n",
    "\n",
    "    # predicted BC operators\n",
    "    u_n_bc, lap_n_bc = compute_normal_derivatives(\n",
    "        x_bc, y_bc,\n",
    "        u_x_bc, u_y_bc,\n",
    "        u_xx_bc, u_yy_bc, u_xy_bc,\n",
    "        u_xxx_bc, u_xxy_bc, u_xyy_bc, u_yyy_bc\n",
    "    )\n",
    "\n",
    "    # exact BC from exact u\n",
    "    u_exact_bc = exact_solution1(x_bc, y_bc)\n",
    "\n",
    "    u_exact_x_bc = torch.autograd.grad(u_exact_bc, x_bc,\n",
    "                                      grad_outputs=torch.ones_like(u_exact_bc),\n",
    "                                      create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_y_bc = torch.autograd.grad(u_exact_bc, y_bc,\n",
    "                                      grad_outputs=torch.ones_like(u_exact_bc),\n",
    "                                      create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    n_x_bc, n_y_bc = compute_normals(x_bc, y_bc)\n",
    "    u_n_exact_bc = n_x_bc * u_exact_x_bc + n_y_bc * u_exact_y_bc\n",
    "\n",
    "    u_exact_xx_bc = torch.autograd.grad(u_exact_x_bc, x_bc,\n",
    "                                       grad_outputs=torch.ones_like(u_exact_x_bc),\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "    u_exact_yy_bc = torch.autograd.grad(u_exact_y_bc, y_bc,\n",
    "                                       grad_outputs=torch.ones_like(u_exact_y_bc),\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "    lap_exact_bc = u_exact_xx_bc + u_exact_yy_bc\n",
    "\n",
    "    lap_x_exact = torch.autograd.grad(lap_exact_bc, x_bc,\n",
    "                                     grad_outputs=torch.ones_like(lap_exact_bc),\n",
    "                                     create_graph=True, retain_graph=True)[0]\n",
    "    lap_y_exact = torch.autograd.grad(lap_exact_bc, y_bc,\n",
    "                                     grad_outputs=torch.ones_like(lap_exact_bc),\n",
    "                                     create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    lap_n_exact_bc = n_x_bc * lap_x_exact + n_y_bc * lap_y_exact\n",
    "\n",
    "    loss_bc = torch.mean((u_n_bc - u_n_exact_bc) ** 2) + \\\n",
    "              torch.mean((lap_n_bc - lap_n_exact_bc) ** 2)\n",
    "\n",
    "    # --- CORRECTED: Added lambda weighting to balance loss terms ---\n",
    "    lambda_int = 1.0\n",
    "    lambda_bc = 100.0  # Force optimizer to prioritize boundary\n",
    "    total_loss = (lambda_int * loss_int) + (lambda_bc * loss_bc)\n",
    "    # --- END CORRECTION ---\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss, loss_int, loss_bc\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Example 1: u = (1/(2π²)) sin(πx) sin(πy) with P4 BCs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, loss_int, loss_bc = train_step()\n",
    "\n",
    "    # Learning rate scheduler (use scalar loss)\n",
    "    scheduler.step(total_loss.item())\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        # compute monitoring errors (need gradients -> no torch.no_grad())\n",
    "        x_eval = x_test\n",
    "        y_eval = y_test\n",
    "        X_eval = torch.cat([x_eval, y_eval], dim=1)\n",
    "\n",
    "        u_pred = pinn(X_eval)\n",
    "        u_exact = exact_solution1(x_eval, y_eval)\n",
    "\n",
    "        l2_err, en_err, l2_rel, en_rel = compute_errors(\n",
    "            u_pred, u_exact, x_eval, y_eval\n",
    "        )\n",
    "\n",
    "        loss_history.append(total_loss.item())\n",
    "        int_loss_history.append(loss_int.item())\n",
    "        bc_loss_history.append(loss_bc.item())\n",
    "        l2_error_history.append(l2_err.item())\n",
    "        energy_error_history.append(en_err.item())\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(f\"Epoch {epoch:5d} | \"\n",
    "              f\"Total: {total_loss.item():.3e} | \"\n",
    "              f\"Int: {loss_int.item():.3e} | \"\n",
    "              f\"BC: {loss_bc.item():.3e} | \"\n",
    "              f\"L2: {l2_err.item():.3e} | \"\n",
    "              f\"Energy: {en_err.item():.3e} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining time (Example 1): {training_time:.2f} s\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Post-processing: plots and model save\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# grid for visualization (no gradients needed here)\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "y_plot = np.linspace(0, 1, 100)\n",
    "X_plot, Y_plot = np.meshgrid(x_plot, y_plot)\n",
    "X_flat = X_plot.flatten().reshape(-1, 1)\n",
    "Y_flat = Y_plot.flatten().reshape(-1, 1)\n",
    "XY_plot = torch.tensor(np.hstack([X_flat, Y_flat]), dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred_plot = pinn(XY_plot).cpu().numpy().reshape(100, 100)\n",
    "\n",
    "u_exact_plot = exact_solution1(\n",
    "    torch.tensor(X_flat, dtype=torch.float32).to(device),\n",
    "    torch.tensor(Y_flat, dtype=torch.float32).to(device)\n",
    ").detach().cpu().numpy().reshape(100, 100)\n",
    "\n",
    "error_plot = np.abs(u_pred_plot - u_exact_plot)\n",
    "\n",
    "# 1) loss history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(loss_history, label=\"Total Loss\", linewidth=2)\n",
    "plt.semilogy(int_loss_history, label=\"Interior Loss\", linewidth=2)\n",
    "plt.semilogy(bc_loss_history, label=\"Boundary Loss\", linewidth=2)\n",
    "plt.xlabel(\"Index (every 100 epochs)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Example 1: Training Loss History (P4)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(results_dir, \"example1_loss_history.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 2) error history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(l2_error_history, label=\"L2 Error\", linewidth=2)\n",
    "plt.semilogy(energy_error_history, label=\"Energy Error\", linewidth=2)\n",
    "plt.xlabel(\"Index (every 100 epochs)\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Example 1: Error History (P4)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(os.path.join(results_dir, \"example1_error_history.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 3) predicted solution\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, u_pred_plot, levels=50, cmap=\"viridis\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: PINN Predicted Solution (P4)\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_predicted_solution.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 4) exact solution\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, u_exact_plot, levels=50, cmap=\"viridis\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: Exact Solution\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_exact_solution.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 5) absolute error\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(X_plot, Y_plot, error_plot, levels=50, cmap=\"hot\")\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Example 1: Absolute Error |u - uθ|\")\n",
    "plt.savefig(os.path.join(results_dir, \"example1_absolute_error.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# save model\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": pinn.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"training_time\": training_time,\n",
    "    },\n",
    "    os.path.join(results_dir, \"example1_model.pth\"),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Final error evaluation (NEEDS gradients -> no torch.no_grad)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "x_eval = torch.rand((2000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "y_eval = torch.rand((2000, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "X_eval = torch.cat([x_eval, y_eval], dim=1)\n",
    "\n",
    "u_pred_final = pinn(X_eval)\n",
    "u_exact_final = exact_solution1(x_eval, y_eval)\n",
    "\n",
    "l2_error_final, energy_error_final, l2_rel_final, energy_rel_final = compute_errors(\n",
    "    u_pred_final, u_exact_final, x_eval, y_eval\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS - EXAMPLE 1 (P4)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"L2 Error            : {l2_error_final.item():.6e}\")\n",
    "print(f\"Energy Error        : {energy_error_final.item():.6e}\")\n",
    "print(f\"Relative L2 Error   : {l2_rel_final.item():.6e}\")\n",
    "print(f\"Relative H1 Error   : {energy_rel_final.item():.6e}\")\n",
    "print(f\"All plots & model saved in: {results_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f1cb1-6ddd-4f20-b3a1-bcb73d2f4a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
